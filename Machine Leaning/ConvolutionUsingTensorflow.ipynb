{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionUsingTensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonudoo/DSA/blob/master/Machine%20Leaning/ConvolutionUsingTensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt3uaFUkwus2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhQYemwoyJ4U",
        "colab_type": "code",
        "outputId": "e5e961c6-8bd5-4a0a-dc03-6e462e30e2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvkTRotAyRBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.load(\"/content/drive/My Drive/Colab Notebooks/datasets/SIGNS/X.npy\")\n",
        "Y = np.load(\"/content/drive/My Drive/Colab Notebooks/datasets/SIGNS/Y.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peI02rkMyfjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__RN_JUpyl09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = X.shape[0]\n",
        "m_train = int(m * 0.8)\n",
        "m_test = m - m_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXw8zxi8yoW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rperm = np.random.permutation(m)\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "for i in range(m_train):\n",
        "    X_train.append(X[rperm[i]])\n",
        "    Y_train.append(Y[rperm[i]])\n",
        "\n",
        "for i in range(m_train, m):\n",
        "    X_test.append(X[rperm[i]])\n",
        "    Y_test.append(Y[rperm[i]])\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float64)\n",
        "X_test = np.array(X_test, dtype=np.float64)\n",
        "Y_train = np.array(Y_train, dtype=np.float64)\n",
        "Y_test = np.array(Y_test, dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnFPbPPdysb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float64, [None, X_train.shape[1], X_train.shape[2], X_train.shape[3]])\n",
        "Y = tf.placeholder(tf.float64, [None, Y_train.shape[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59CemEfjyuEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W1 = tf.Variable(np.random.rand(4, 4, X_train.shape[3], 8), 'W1', dtype=tf.float64)\n",
        "W2 = tf.Variable(np.random.rand(2, 2, 8, 16), 'W2', dtype=tf.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStRf6WQyvwl",
        "colab_type": "code",
        "outputId": "1c9dbe98-e556-46e7-908d-b75a2098d13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "A1 = tf.nn.relu(Z1)\n",
        "P1 = tf.nn.max_pool(A1, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], padding='SAME')\n",
        "Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "A2 = tf.nn.relu(Z2)\n",
        "P2 = tf.nn.max_pool(A2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')\n",
        "F2 = tf.contrib.layers.flatten(P2)\n",
        "Z3 = tf.contrib.layers.fully_connected(F2, 10, activation_fn=None)\n",
        "J = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(0.009).minimize(J)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fb6dba0e518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fb6dba0e518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fb6dba0e518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fb6dba0e518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb6dba0a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb6dba0a048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb6dba0a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb6dba0a048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-9-e44b7746422c>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CVb7YAgzX8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "n_complete_batches = m_train // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0uqEmjcyxKZ",
        "colab_type": "code",
        "outputId": "38908e4b-688c-4dca-cce6-d36a86425cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "for i in range(500):\n",
        "    for j in range(n_complete_batches):\n",
        "        sess.run([optimizer, J], {X: X_train[j*batch_size: (j+1)*batch_size], Y: Y_train[j*batch_size: (j+1)*batch_size]})\n",
        "    _, J_opt = sess.run([optimizer, J], {X: X_train[j*n_complete_batches:], Y: Y_train[j*n_complete_batches:]})\n",
        "    print(i, J_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 163.80477561108935\n",
            "1 55.11156275459979\n",
            "2 12.766942765875028\n",
            "3 5.241255142854731\n",
            "4 4.133831950173293\n",
            "5 3.72876726127107\n",
            "6 3.484261512535076\n",
            "7 3.144592056937692\n",
            "8 3.02171226324614\n",
            "9 3.0044856926008228\n",
            "10 2.85002856620125\n",
            "11 2.6426428372708717\n",
            "12 2.6202378443689813\n",
            "13 2.4970759833511558\n",
            "14 2.4648716910220787\n",
            "15 2.416964395386517\n",
            "16 2.427047957141529\n",
            "17 2.4397129474226045\n",
            "18 2.423721470025882\n",
            "19 2.391918844925482\n",
            "20 2.3672765187930453\n",
            "21 2.320318592923153\n",
            "22 2.271290795050888\n",
            "23 2.237900277132765\n",
            "24 2.212409641524281\n",
            "25 2.1879170685981912\n",
            "26 2.1644402719643914\n",
            "27 2.141086283330846\n",
            "28 2.1164424816698126\n",
            "29 2.0932217751721787\n",
            "30 2.069890924663357\n",
            "31 2.04512188013046\n",
            "32 2.0192791822588982\n",
            "33 1.991919353213528\n",
            "34 1.9637513256095132\n",
            "35 1.9341075164765698\n",
            "36 1.9043839323135323\n",
            "37 1.8733149826943296\n",
            "38 1.8430139722844565\n",
            "39 1.813170154043177\n",
            "40 1.7866386337357478\n",
            "41 1.7634055388471228\n",
            "42 1.7412405053720277\n",
            "43 1.7204531904589635\n",
            "44 1.6928138334372425\n",
            "45 1.6610208845844172\n",
            "46 1.619467535220782\n",
            "47 1.5757632683217793\n",
            "48 1.5318633628383382\n",
            "49 1.4901134510991019\n",
            "50 1.4469392569012982\n",
            "51 1.3974487774646556\n",
            "52 1.3484326097097161\n",
            "53 1.311927655018987\n",
            "54 1.277128758595039\n",
            "55 1.24756708415558\n",
            "56 1.2231865376587119\n",
            "57 1.1966533030186688\n",
            "58 1.1695026951125704\n",
            "59 1.1467768033404895\n",
            "60 1.1282876655436278\n",
            "61 1.1105787425150724\n",
            "62 1.0945473197564044\n",
            "63 1.0780438256191396\n",
            "64 1.062859319894029\n",
            "65 1.04693951900415\n",
            "66 1.0314478488602985\n",
            "67 1.0174687882925562\n",
            "68 1.002785998268503\n",
            "69 0.9902536164290675\n",
            "70 0.978411665942207\n",
            "71 0.9652588054735548\n",
            "72 0.9543060987076324\n",
            "73 0.9452252939095677\n",
            "74 0.9341605017238134\n",
            "75 0.9249565904446525\n",
            "76 0.9145527747593332\n",
            "77 0.9045893879638209\n",
            "78 0.8952074110310029\n",
            "79 0.8863221857245155\n",
            "80 0.8779327625086286\n",
            "81 0.8693734775830718\n",
            "82 0.8609495757749628\n",
            "83 0.8517602768500357\n",
            "84 0.8433321403929354\n",
            "85 0.8358713429419871\n",
            "86 0.8298551952932864\n",
            "87 0.8213269504528935\n",
            "88 0.812497280943008\n",
            "89 0.8038050918186831\n",
            "90 0.795370815358578\n",
            "91 0.786442281310193\n",
            "92 0.77810333795838\n",
            "93 0.7696483941218251\n",
            "94 0.760760805024444\n",
            "95 0.7515099975828312\n",
            "96 0.7428651059147949\n",
            "97 0.7344779834520191\n",
            "98 0.7259224158427019\n",
            "99 0.7173039141084462\n",
            "100 0.7086336717944739\n",
            "101 0.7000654460589388\n",
            "102 0.6919896192601328\n",
            "103 0.6837057325795509\n",
            "104 0.6752623711951721\n",
            "105 0.6668774189466329\n",
            "106 0.6575810329838307\n",
            "107 0.6493577448426042\n",
            "108 0.6408847028709665\n",
            "109 0.6327230178224369\n",
            "110 0.6247808844724909\n",
            "111 0.6171081408157634\n",
            "112 0.6098001399706361\n",
            "113 0.6019407220923795\n",
            "114 0.5938136977538131\n",
            "115 0.5861586129661956\n",
            "116 0.5786485356336304\n",
            "117 0.5714179047118954\n",
            "118 0.5642821766748839\n",
            "119 0.5563462537346686\n",
            "120 0.549712490254728\n",
            "121 0.5420908594924494\n",
            "122 0.5348821235847464\n",
            "123 0.5281956432148663\n",
            "124 0.5222392038718334\n",
            "125 0.5156407373206222\n",
            "126 0.5096074504186459\n",
            "127 0.5035901886677207\n",
            "128 0.49564439067363547\n",
            "129 0.4906374097889861\n",
            "130 0.4851035131735262\n",
            "131 0.4782649657810519\n",
            "132 0.4723006364550234\n",
            "133 0.468258296275852\n",
            "134 0.46280841102047876\n",
            "135 0.45637591545976963\n",
            "136 0.45075725695829383\n",
            "137 0.446957371810794\n",
            "138 0.4404964053515759\n",
            "139 0.4359261074248897\n",
            "140 0.4313247639423451\n",
            "141 0.42634832871411676\n",
            "142 0.4223109717992692\n",
            "143 0.4194245198319939\n",
            "144 0.41544558937036385\n",
            "145 0.4116141854186561\n",
            "146 0.4068977565640268\n",
            "147 0.4041058306469898\n",
            "148 0.3994396418099507\n",
            "149 0.3966038767821718\n",
            "150 0.3908907615394275\n",
            "151 0.3875128985937736\n",
            "152 0.3828927770142218\n",
            "153 0.3783453594867337\n",
            "154 0.37350385713468304\n",
            "155 0.3694166103343162\n",
            "156 0.3614900403305354\n",
            "157 0.35807682856555534\n",
            "158 0.3522948938075274\n",
            "159 0.35128706572189244\n",
            "160 0.34388772071356555\n",
            "161 0.34533524664539184\n",
            "162 0.33640422005107745\n",
            "163 0.331977613479096\n",
            "164 0.3294545955059356\n",
            "165 0.32736678485929877\n",
            "166 0.3228821882019486\n",
            "167 0.31654485711327773\n",
            "168 0.31151361640796044\n",
            "169 0.309500874281249\n",
            "170 0.3087696531100165\n",
            "171 0.30490629167415284\n",
            "172 0.30479821150862907\n",
            "173 0.29708381819721197\n",
            "174 0.2929147498547225\n",
            "175 0.29496586809299463\n",
            "176 0.2890467660594624\n",
            "177 0.29344869241474136\n",
            "178 0.2917727874711179\n",
            "179 0.2920380089945116\n",
            "180 0.2893663251243286\n",
            "181 0.30141200765020776\n",
            "182 0.28973429843985166\n",
            "183 0.29009494701956906\n",
            "184 0.2915417836851998\n",
            "185 0.2850395517771495\n",
            "186 0.2776932497882807\n",
            "187 0.28177432337764413\n",
            "188 0.2844851413327349\n",
            "189 0.27657033835334444\n",
            "190 0.2797206302860976\n",
            "191 0.2865316276783599\n",
            "192 0.27053675592180904\n",
            "193 0.2728094834223677\n",
            "194 0.28107901090942383\n",
            "195 0.26757832247107977\n",
            "196 0.27502060948083257\n",
            "197 0.27489094857957064\n",
            "198 0.27618704330876853\n",
            "199 0.26668302624261214\n",
            "200 0.26645919274901353\n",
            "201 0.275903094471401\n",
            "202 0.2680992500238013\n",
            "203 0.27662599580739095\n",
            "204 0.26938441947893854\n",
            "205 0.2759923692952989\n",
            "206 0.2623766367863409\n",
            "207 0.29629134028947796\n",
            "208 0.28097736786512306\n",
            "209 0.2676692809624126\n",
            "210 0.3287140810206496\n",
            "211 0.3014379917364316\n",
            "212 0.26891793668400166\n",
            "213 0.3587818055038695\n",
            "214 0.3010549193681581\n",
            "215 0.3067123346057349\n",
            "216 0.3164816045198351\n",
            "217 0.28251258779598304\n",
            "218 0.2981896266593146\n",
            "219 0.2718182853863047\n",
            "220 0.25460363461371155\n",
            "221 0.2509372021561963\n",
            "222 0.20787358567309108\n",
            "223 0.22865537364170974\n",
            "224 0.2140708251215679\n",
            "225 0.17914497850655314\n",
            "226 0.28996479606588965\n",
            "227 0.26303503344504564\n",
            "228 0.2700663872950266\n",
            "229 0.30574997418915845\n",
            "230 0.1871408719697908\n",
            "231 0.156519268999943\n",
            "232 0.1677467725846259\n",
            "233 0.15473613850330462\n",
            "234 0.14230661798928504\n",
            "235 0.1283881678929626\n",
            "236 0.13122902444470164\n",
            "237 0.12204425489204687\n",
            "238 0.16008415976393822\n",
            "239 0.20826776390443846\n",
            "240 0.23778053485182205\n",
            "241 0.24234906460975364\n",
            "242 0.22445572556893917\n",
            "243 0.19294154504909194\n",
            "244 0.17010122017223625\n",
            "245 0.19168182258484986\n",
            "246 0.1707623826765652\n",
            "247 0.19067908321701807\n",
            "248 0.17981541405292165\n",
            "249 0.17518238456167412\n",
            "250 0.1730020927305024\n",
            "251 0.14586836144363466\n",
            "252 0.14746502165950265\n",
            "253 0.17272853661105186\n",
            "254 0.11646659558633454\n",
            "255 0.20251891077406323\n",
            "256 0.1432033171371123\n",
            "257 0.3085113482972993\n",
            "258 0.26742219670660305\n",
            "259 0.23838213527838187\n",
            "260 0.20809808952582107\n",
            "261 0.1944902051886109\n",
            "262 0.22408835019748105\n",
            "263 0.25609409429794505\n",
            "264 0.14429397632786364\n",
            "265 0.1843288009923483\n",
            "266 0.13085298957624159\n",
            "267 0.2299742662054279\n",
            "268 0.2523783604932189\n",
            "269 0.214851780992526\n",
            "270 0.17608616384118975\n",
            "271 0.1310048841566123\n",
            "272 0.17332029426132164\n",
            "273 0.29457518842141617\n",
            "274 0.35553627394559617\n",
            "275 0.2648171853584692\n",
            "276 0.2949941289249239\n",
            "277 0.5156720935938629\n",
            "278 0.37034722487135296\n",
            "279 0.19184499665803617\n",
            "280 0.2557286210948481\n",
            "281 0.19616935764307714\n",
            "282 0.23515022077449535\n",
            "283 0.1957941980836594\n",
            "284 0.24564019194842096\n",
            "285 0.2055842126741724\n",
            "286 0.19502593961594633\n",
            "287 0.16678939596414286\n",
            "288 0.1882859289460684\n",
            "289 0.18618625009050507\n",
            "290 0.19705587834502503\n",
            "291 0.1821173574807005\n",
            "292 0.21095026259049393\n",
            "293 0.17484092797118284\n",
            "294 0.15980657638232018\n",
            "295 0.13469290626882147\n",
            "296 0.17454466750007566\n",
            "297 0.1318429001247259\n",
            "298 0.11055660502540675\n",
            "299 0.14166840583610285\n",
            "300 0.14960854863633036\n",
            "301 0.07510224165496773\n",
            "302 0.10083825879259507\n",
            "303 0.1266303566443689\n",
            "304 0.08631228992425567\n",
            "305 0.09627208149315965\n",
            "306 0.14948641134478038\n",
            "307 0.15123401021556676\n",
            "308 0.19199031957745652\n",
            "309 0.11565990197013253\n",
            "310 0.13726117260432896\n",
            "311 0.23786412501252172\n",
            "312 0.1583705550396447\n",
            "313 0.12335132777901482\n",
            "314 0.1063368613552306\n",
            "315 0.14187230186399613\n",
            "316 0.09278059386554836\n",
            "317 0.10429803635591411\n",
            "318 0.08467253558218749\n",
            "319 0.12302990745476393\n",
            "320 0.10963464175283276\n",
            "321 0.09023750013480862\n",
            "322 0.06244991603594977\n",
            "323 0.07627904161008284\n",
            "324 0.08803028011543917\n",
            "325 0.11205310282641966\n",
            "326 0.08514387902877689\n",
            "327 0.06928961824438323\n",
            "328 0.0848511935982519\n",
            "329 0.13264531717818792\n",
            "330 0.06198052411912833\n",
            "331 0.08570491693836683\n",
            "332 0.11943866822458356\n",
            "333 0.09864470774090787\n",
            "334 0.15164227706529532\n",
            "335 0.19313586806757424\n",
            "336 0.08702114919881897\n",
            "337 0.11454805533820898\n",
            "338 0.18058245085228644\n",
            "339 0.09094473698623722\n",
            "340 0.11774763424281022\n",
            "341 0.08656333714200241\n",
            "342 0.1195263249736359\n",
            "343 0.13551832513257325\n",
            "344 0.06690947679658135\n",
            "345 0.0670896203090482\n",
            "346 0.04739446571033744\n",
            "347 0.17165484586061092\n",
            "348 0.04485577123126985\n",
            "349 0.06635001938613667\n",
            "350 0.20244522457585612\n",
            "351 0.20635796750488547\n",
            "352 0.07756632637197017\n",
            "353 0.14973973376935806\n",
            "354 0.044750687570316416\n",
            "355 0.10550714066339996\n",
            "356 0.11498413479715917\n",
            "357 0.06761582748988831\n",
            "358 0.2375952594746845\n",
            "359 0.14984120907222334\n",
            "360 0.07509626683771083\n",
            "361 0.03675008946571191\n",
            "362 0.1647673897561862\n",
            "363 0.0566577062603146\n",
            "364 0.04195827670898191\n",
            "365 0.1378280323018851\n",
            "366 0.21777724175603652\n",
            "367 0.0378542734302268\n",
            "368 0.15371900184896245\n",
            "369 0.04811887907276397\n",
            "370 0.049569949101662056\n",
            "371 0.16791416360238787\n",
            "372 0.07633892986420611\n",
            "373 0.1757035254289743\n",
            "374 0.06545982135980448\n",
            "375 0.06180925070031685\n",
            "376 0.20391812456548125\n",
            "377 0.06915436124893716\n",
            "378 0.14417565199885404\n",
            "379 0.08166179359063312\n",
            "380 0.06477608100254871\n",
            "381 0.1411596881732314\n",
            "382 0.08125932476588568\n",
            "383 0.1204919921187582\n",
            "384 0.20353947448559753\n",
            "385 0.3010087144993442\n",
            "386 0.3817953680768121\n",
            "387 0.13194041166341636\n",
            "388 0.06442391919166433\n",
            "389 0.04099421753281058\n",
            "390 0.05148030864736987\n",
            "391 0.08298709242689037\n",
            "392 0.06831264819284322\n",
            "393 0.043191150140113474\n",
            "394 0.036531356150135766\n",
            "395 0.04045002143498659\n",
            "396 0.03398333344729385\n",
            "397 0.027128640358428883\n",
            "398 0.027465954177868915\n",
            "399 0.022449641744334694\n",
            "400 0.033225466413336756\n",
            "401 0.03836558555617025\n",
            "402 0.03341451255446311\n",
            "403 0.04322292791998368\n",
            "404 0.02105000476868547\n",
            "405 0.030097953114790764\n",
            "406 0.07556522893625478\n",
            "407 0.0565700975473148\n",
            "408 0.062375319560793265\n",
            "409 0.07717449007499433\n",
            "410 0.03561903016244786\n",
            "411 0.04165425343994442\n",
            "412 0.03537281215899548\n",
            "413 0.06442087947362327\n",
            "414 0.03062992772113245\n",
            "415 0.04404527334598952\n",
            "416 0.04151434168228118\n",
            "417 0.05252124972682661\n",
            "418 0.027371767025718505\n",
            "419 0.044498491204253386\n",
            "420 0.05074970577946066\n",
            "421 0.05902330619120895\n",
            "422 0.04753791866851706\n",
            "423 0.0461212186467751\n",
            "424 0.040302597214302974\n",
            "425 0.10802736649977723\n",
            "426 0.024298253783550456\n",
            "427 0.06162620829197758\n",
            "428 0.03560814936993108\n",
            "429 0.024811645401960014\n",
            "430 0.14945004408496548\n",
            "431 0.11463328276167721\n",
            "432 0.1031033584628525\n",
            "433 0.07684740329926046\n",
            "434 0.28522245344795066\n",
            "435 0.09460859460972589\n",
            "436 0.04898416999374204\n",
            "437 0.048136212087813234\n",
            "438 0.033213995467724126\n",
            "439 0.07133106101236693\n",
            "440 0.0331342792201807\n",
            "441 0.03885872160674148\n",
            "442 0.026389297736357644\n",
            "443 0.014590754683852666\n",
            "444 0.04988459773466504\n",
            "445 0.010016896838677061\n",
            "446 0.01057921803204969\n",
            "447 0.006784123770121693\n",
            "448 0.004329348462581523\n",
            "449 0.003572887143281386\n",
            "450 0.002569970531895603\n",
            "451 0.0021081765533831703\n",
            "452 0.0019265888441558944\n",
            "453 0.001771856905296229\n",
            "454 0.001667324931046286\n",
            "455 0.0015779308975399297\n",
            "456 0.0015093979209062044\n",
            "457 0.00145649951370272\n",
            "458 0.0014114754584863035\n",
            "459 0.001366376249631979\n",
            "460 0.001331948779510295\n",
            "461 0.0012992281513213486\n",
            "462 0.0012651612909474789\n",
            "463 0.0012359809794811342\n",
            "464 0.00121012195420572\n",
            "465 0.001185413858193287\n",
            "466 0.0011627372938350098\n",
            "467 0.0011364678282665265\n",
            "468 0.0011084516036001923\n",
            "469 0.0010861976458871546\n",
            "470 0.001063230686333807\n",
            "471 0.0010362561045648277\n",
            "472 0.0010113247738914937\n",
            "473 0.0009892037788284016\n",
            "474 0.000967128012347936\n",
            "475 0.0009461627512922412\n",
            "476 0.0009229039165167633\n",
            "477 0.000901211884655774\n",
            "478 0.0008815087852965327\n",
            "479 0.0008637852129732314\n",
            "480 0.0008465104719455922\n",
            "481 0.0008286747035634624\n",
            "482 0.0008116716996393863\n",
            "483 0.0007956770932631723\n",
            "484 0.0007812053084440148\n",
            "485 0.0007670268452687897\n",
            "486 0.0007522404645021915\n",
            "487 0.000737156069020573\n",
            "488 0.0007232425087908995\n",
            "489 0.0007073145763555558\n",
            "490 0.0006935284621883707\n",
            "491 0.00067850085447921\n",
            "492 0.0006658008553448893\n",
            "493 0.0006541511171398097\n",
            "494 0.000642657073849748\n",
            "495 0.0006321439843136067\n",
            "496 0.00061979192405477\n",
            "497 0.0006090838855724723\n",
            "498 0.0005993520185233836\n",
            "499 0.0005932188449942442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhEUELfMBvfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = sess.run([Z3], {X: X_train, Y: Y_train})[0]\n",
        "Y_pred_test = sess.run([Z3], {X: X_test, Y: Y_test})[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIzM2dHBw5i",
        "colab_type": "code",
        "outputId": "7644e417-4e8d-4dc3-e295-374e3baa82b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Training accuracy:\", np.sum(np.argmax(Y_train, axis=1) == np.argmax(Y_pred, axis=1)) / m_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJwasWXgz4gS",
        "colab_type": "code",
        "outputId": "921916d4-49b6-4843-917d-b99b157ef0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Test accuracy:\", np.sum(np.argmax(Y_test, axis=1) == np.argmax(Y_pred_test, axis=1)) / m_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9249394673123487\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}