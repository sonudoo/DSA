{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VariableLengthLSTMUsingTensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonudoo/DSA/blob/master/Machine%20Learning/VariableLengthLSTMUsingTensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrbWmPtHImIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEPMrnEDIr-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mapping of characters to index and vice-versa\n",
        "\n",
        "d = {0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
        "d_reversed = {}\n",
        "for key in d:\n",
        "    d_reversed[d[key]] = key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG0g-oY6IwiB",
        "colab_type": "code",
        "outputId": "59a730ac-ec85-4936-d50b-55adb82b541f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7CoBA15JGEM",
        "colab_type": "code",
        "outputId": "9648d7cc-4090-42eb-a2b6-6494b773a560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data = [name.lower() + '\\n' for name in open(\"/content/gdrive/My Drive/Colab Notebooks/datasets/DINOS/dinos.txt\", \"r\").read().split()]\n",
        "np.random.shuffle(data)\n",
        "print(\"Longest name is of length:\", max([len(x) for x in data]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest name is of length: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzeMJ3THJScz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will be generating dinosaur names of variable length.\n",
        "# Since the dinosaur names are of variable length, we define maximum states\n",
        "\n",
        "max_states = 27 # No more than 27 length names present\n",
        "n_input = 27 # Number of units in input layer (26 alphabets + 1 EON character)\n",
        "n_hidden = 100 # Number of units in hidden layer\n",
        "n_output = 27 # Number of units in output layer (26 alphabets + 1 EON character)\n",
        "m = len(data)\n",
        "\n",
        "# Since the length is variable, we will be taking only a single example at a time to train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg8XuAS2JXJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X will be of dimension (m, l, n_input) m = Number of training example, l = length of the sequence, \n",
        "# We post-pad all sequences of length less than max_states with '\\n'\n",
        "# The output for each character is the next character. The output for '\\n' is always '\\n'\n",
        "# We will be running one-hot encoding for each character\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for i in range(m):\n",
        "    \n",
        "    data[i] = data[i] + '\\n' * (max_states - len(data[i]))\n",
        "    \n",
        "    input_string = data[i]\n",
        "    output_string = data[i][1: ] + '\\n'\n",
        "    \n",
        "    input_encoded = []\n",
        "    for j in input_string:\n",
        "        input_encoded.append([0.0 if k != d_reversed[j] else 1.0 for k in range(n_input)])\n",
        "    input_encoded = np.array(input_encoded)\n",
        "    \n",
        "    output_encoded = []\n",
        "    for j in output_string:\n",
        "        output_encoded.append([0.0 if k != d_reversed[j] else 1.0 for k in range(n_input)])\n",
        "    output_encoded = np.array(output_encoded)\n",
        "    \n",
        "    X_train.append(input_encoded)\n",
        "    Y_train.append(output_encoded)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8GK-2OMJZQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert(X_train.shape == Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi8aMu5VJbGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No need of batch size here, we will be training only one example at a time\n",
        "\n",
        "X = tf.placeholder(dtype=tf.float32, shape=(1, max_states, n_input))\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=(1, max_states, n_output))\n",
        "\n",
        "# Loss mask is used for calculating loss for variable length states.\n",
        "# We set the mask value to 1 for all states less than the state length of the example and 0 for all others\n",
        "# This loss mask would be multiplied by loss vector and summed over to get the total loss for training example\n",
        "\n",
        "loss_mask = tf.placeholder(dtype=tf.float32, shape=(max_states, 1))\n",
        "\n",
        "# Hidden layer\n",
        "\n",
        "W_c = tf.Variable(tf.random_normal([n_hidden + n_input, n_hidden]))\n",
        "W_u = tf.Variable(tf.random_normal([n_hidden + n_input, n_hidden]))\n",
        "W_f = tf.Variable(tf.random_normal([n_hidden + n_input, n_hidden]))\n",
        "W_o = tf.Variable(tf.random_normal([n_hidden + n_input, n_hidden]))\n",
        "\n",
        "b_c = tf.Variable(tf.random_normal([1, n_hidden]))\n",
        "b_u = tf.Variable(tf.random_normal([1, n_hidden]))\n",
        "b_f = tf.Variable(tf.random_normal([1, n_hidden]))\n",
        "b_o = tf.Variable(tf.random_normal([1, n_hidden]))\n",
        "\n",
        "# Output layer\n",
        "\n",
        "W_y = tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
        "b_y = tf.Variable(tf.random_normal([1, n_output]))\n",
        "\n",
        "# Again, only one training example at a time\n",
        "\n",
        "a = tf.zeros(dtype=tf.float32, shape=[1, n_hidden]) \n",
        "c = tf.zeros(dtype=tf.float32, shape=[1, n_hidden])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqd_6-pJ6HQ",
        "colab_type": "code",
        "outputId": "da558f2d-6052-4414-efb8-ebe4499f4efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "all_states_input = tf.unstack(X[0], axis=0) # Returns a list of tensors sliced column wise from X[0] (Only one training example)\n",
        "all_states_label = tf.unstack(Y[0], axis=0) # Returns a list of tensors sliced column wise from Y[0] (Only one training example)\n",
        "losses = []\n",
        "\n",
        "for ith_state in range(max_states):\n",
        "    \n",
        "    # We extract the ith state input and label\n",
        "    X_state_input = tf.reshape(all_states_input[ith_state], shape=[1, n_input])\n",
        "    Y_state_label = tf.reshape(all_states_label[ith_state], shape=[1, n_output])\n",
        "    \n",
        "    # Stack it with the previous state\n",
        "    a_prev_x_stacked = tf.concat([a, X_state_input], 1)\n",
        "        \n",
        "    # Forward prop\n",
        "    \n",
        "    _c = tf.nn.tanh(tf.math.add(tf.matmul(a_prev_x_stacked, W_c), b_c))\n",
        "    g_u = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_u), b_u))\n",
        "    g_f = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_f), b_f))\n",
        "    g_o = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_o), b_o))\n",
        "    c = tf.math.add(tf.math.multiply(g_u, _c), tf.math.multiply(g_f, c))\n",
        "    a = tf.math.multiply(g_o, tf.nn.tanh(c))\n",
        "    Y_state_output = tf.math.add(tf.matmul(a, W_y), b_y)\n",
        "        \n",
        "    # Calculate the loss for this state\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=Y_state_output, labels=Y_state_label)[0]\n",
        "    \n",
        "    losses.append(loss)\n",
        "    \n",
        "# We take the sum of losses of non-padded characters only\n",
        "\n",
        "total_loss = tf.reduce_sum(tf.multiply(tf.reshape(tf.convert_to_tensor(losses), shape=[max_states, 1]), loss_mask))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-f8fe06b5abe0>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EjNOOOrJ8Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Kw-4hEJ-q1",
        "colab_type": "code",
        "outputId": "ab606005-cf59-4f19-c0d1-921da46312c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's learn for 50 epochs\n",
        "\n",
        "for epoch in range(50):\n",
        "    loss_across_batches = []\n",
        "    for i in range(X_train.shape[0]):\n",
        "        loss_mask_array = np.array([1.0 if j < len(data[i].split('\\n')[0]) else 0.0 for j in range(max_states)]).reshape(max_states, 1)\n",
        "        loss = sess.run([optimizer, total_loss], feed_dict={X: [X_train[i]], Y: [Y_train[i]], loss_mask: loss_mask_array})[1]\n",
        "        loss_across_batches.append(loss)\n",
        "        print('\\rEpoch:', epoch + 1, 'Ex:', i + 1, 'L:', round(loss), end='')\n",
        "    print('\\nOverall loss in Epoch:', epoch + 1, 'is', np.mean(np.array(loss_across_batches)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Ex: 1536 L: 13.0\n",
            "Overall loss in Epoch: 1 is 37.878246\n",
            "Epoch: 2 Ex: 1536 L: 14.0\n",
            "Overall loss in Epoch: 2 is 24.399221\n",
            "Epoch: 3 Ex: 1536 L: 14.0\n",
            "Overall loss in Epoch: 3 is 22.072067\n",
            "Epoch: 4 Ex: 1536 L: 14.0\n",
            "Overall loss in Epoch: 4 is 20.721678\n",
            "Epoch: 5 Ex: 1536 L: 13.0\n",
            "Overall loss in Epoch: 5 is 19.878742\n",
            "Epoch: 6 Ex: 1536 L: 14.0\n",
            "Overall loss in Epoch: 6 is 19.295713\n",
            "Epoch: 7 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 7 is 18.815449\n",
            "Epoch: 8 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 8 is 18.294321\n",
            "Epoch: 9 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 9 is 17.782557\n",
            "Epoch: 10 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 10 is 17.251635\n",
            "Epoch: 11 Ex: 1536 L: 8.0\n",
            "Overall loss in Epoch: 11 is 16.708853\n",
            "Epoch: 12 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 12 is 16.179726\n",
            "Epoch: 13 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 13 is 15.636093\n",
            "Epoch: 14 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 14 is 15.332886\n",
            "Epoch: 15 Ex: 1536 L: 8.0\n",
            "Overall loss in Epoch: 15 is 14.833701\n",
            "Epoch: 16 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 16 is 14.529179\n",
            "Epoch: 17 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 17 is 14.219855\n",
            "Epoch: 18 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 18 is 13.911248\n",
            "Epoch: 19 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 19 is 13.741489\n",
            "Epoch: 20 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 20 is 13.448132\n",
            "Epoch: 21 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 21 is 13.229846\n",
            "Epoch: 22 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 22 is 12.881894\n",
            "Epoch: 23 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 23 is 12.89075\n",
            "Epoch: 24 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 24 is 12.630432\n",
            "Epoch: 25 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 25 is 12.580178\n",
            "Epoch: 26 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 26 is 12.386639\n",
            "Epoch: 27 Ex: 1536 L: 5.0\n",
            "Overall loss in Epoch: 27 is 12.295293\n",
            "Epoch: 28 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 28 is 12.1345825\n",
            "Epoch: 29 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 29 is 12.025874\n",
            "Epoch: 30 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 30 is 11.958675\n",
            "Epoch: 31 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 31 is 11.941841\n",
            "Epoch: 32 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 32 is 11.851659\n",
            "Epoch: 33 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 33 is 11.718262\n",
            "Epoch: 34 Ex: 1536 L: 12.0\n",
            "Overall loss in Epoch: 34 is 11.744481\n",
            "Epoch: 35 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 35 is 11.669037\n",
            "Epoch: 36 Ex: 1536 L: 12.0\n",
            "Overall loss in Epoch: 36 is 11.530269\n",
            "Epoch: 37 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 37 is 11.361305\n",
            "Epoch: 38 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 38 is 11.5008\n",
            "Epoch: 39 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 39 is 11.209453\n",
            "Epoch: 40 Ex: 1536 L: 7.0\n",
            "Overall loss in Epoch: 40 is 11.392386\n",
            "Epoch: 41 Ex: 1536 L: 5.0\n",
            "Overall loss in Epoch: 41 is 11.167011\n",
            "Epoch: 42 Ex: 1536 L: 11.0\n",
            "Overall loss in Epoch: 42 is 11.293031\n",
            "Epoch: 43 Ex: 1536 L: 9.0\n",
            "Overall loss in Epoch: 43 is 11.0950365\n",
            "Epoch: 44 Ex: 1536 L: 8.0\n",
            "Overall loss in Epoch: 44 is 10.969068\n",
            "Epoch: 45 Ex: 1536 L: 4.0\n",
            "Overall loss in Epoch: 45 is 11.461147\n",
            "Epoch: 46 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 46 is 11.066963\n",
            "Epoch: 47 Ex: 1536 L: 6.0\n",
            "Overall loss in Epoch: 47 is 10.790416\n",
            "Epoch: 48 Ex: 1536 L: 8.0\n",
            "Overall loss in Epoch: 48 is 10.830964\n",
            "Epoch: 49 Ex: 1536 L: 10.0\n",
            "Overall loss in Epoch: 49 is 10.708322\n",
            "Epoch: 50 Ex: 1536 L: 8.0\n",
            "Overall loss in Epoch: 50 is 10.845094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzvU0OyyKBrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's generate a name now. \n",
        "\n",
        "# Build the prediction network, which takes a random input and then generates output of max_states length\n",
        "\n",
        "a = tf.zeros(dtype=tf.float32, shape=[1, n_hidden]) \n",
        "c = tf.zeros(dtype=tf.float32, shape=[1, n_hidden])\n",
        "Y_output = None\n",
        "\n",
        "current_state_input = tf.random_normal(dtype=tf.float32, shape=[1, n_input])\n",
        "\n",
        "for ith_state in range(max_states):\n",
        "    \n",
        "    # Stack it with the previous state\n",
        "    a_prev_x_stacked = tf.concat([a, current_state_input], 1)\n",
        "        \n",
        "    # Forward prop\n",
        "    \n",
        "    _c = tf.nn.tanh(tf.math.add(tf.matmul(a_prev_x_stacked, W_c), b_c))\n",
        "    g_u = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_u), b_u))\n",
        "    g_f = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_f), b_f))\n",
        "    g_o = tf.nn.sigmoid(tf.math.add(tf.matmul(a_prev_x_stacked, W_o), b_o))\n",
        "    c = tf.math.add(tf.math.multiply(g_u, _c), tf.math.multiply(g_f, c))\n",
        "    a = tf.math.multiply(g_o, tf.nn.tanh(c))\n",
        "    Y_state_output = tf.nn.softmax(tf.math.add(tf.matmul(a, W_y), b_y))\n",
        "    if ith_state != 0:\n",
        "        Y_output = tf.concat([Y_output, Y_state_output], 0)\n",
        "    else:\n",
        "        Y_output = Y_state_output\n",
        "    \n",
        "    current_state_input = Y_state_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O8iaxiBL7jc",
        "colab_type": "code",
        "outputId": "d98eb58e-f60b-4ac6-bd97-3d5aac63c9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Print a name\n",
        "\n",
        "\"\".join([d[x] for x in np.argmax(sess.run(Y_output), axis=1)]).split('\\n')[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'madlosaurus'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPHyHDr3S1Uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}